{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMnYGXPazlEwFu7t1XAYSP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtyomIT/Identifying-cats-and-dogs-in-a-photo/blob/main/Identifying_cats_and_dogs_in_a_photo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "!wget https://storage.yandexcloud.net/academy.ai/cat-and-dog.zip\n",
        "!unzip -qo \"cat-and-dog.zip\" -d ./temp"
      ],
      "metadata": {
        "id": "8HRPT8i47310"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNMCk5PhJF-Y",
        "outputId": "de8c4b1d-253f-499f-fbb7-675d9bea0df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-27 15:29:29--  https://storage.yandexcloud.net/academy.ai/cat-and-dog.zip\n",
            "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
            "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228082266 (218M) [application/x-zip-compressed]\n",
            "Saving to: ‘cat-and-dog.zip’\n",
            "\n",
            "cat-and-dog.zip     100%[===================>] 217.52M  19.4MB/s    in 11s     \n",
            "\n",
            "2024-05-27 15:29:41 (19.1 MB/s) - ‘cat-and-dog.zip’ saved [228082266/228082266]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_PATH = './temp/training_set/training_set/'\n",
        "\n",
        "BASE_DIR = './dataset/'\n",
        "\n",
        "CLASS_LIST = sorted(os.listdir(IMAGE_PATH))\n",
        "\n",
        "CLASS_COUNT = len(CLASS_LIST)\n",
        "\n",
        "# Deletes the BASE_DIR if it exists to start fresh\n",
        "if os.path.exists(BASE_DIR):\n",
        "    shutil.rmtree(BASE_DIR)\n",
        "\n",
        "# Creates the BASE_DIR directory\n",
        "os.mkdir(BASE_DIR)\n",
        "\n",
        "# Creates subdirectories for training, validation, and testing datasets\n",
        "train_dir = os.path.join(BASE_DIR, 'train')\n",
        "os.mkdir(train_dir)\n",
        "\n",
        "validation_dir = os.path.join(BASE_DIR, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "\n",
        "test_dir = os.path.join(BASE_DIR, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "# Function to copy images from the source to a destination, used to create datasets\n",
        "def create_dataset(\n",
        "    img_path: str,\n",
        "    new_path: str,\n",
        "    class_name: str,\n",
        "    start_index: int,\n",
        "    end_index: int\n",
        "):\n",
        "    src_path = os.path.join(img_path, class_name)\n",
        "    dst_path = os.path.join(new_path, class_name)\n",
        "    class_files = os.listdir(src_path)\n",
        "    os.mkdir(dst_path)\n",
        "\n",
        "    for fname in class_files[start_index : end_index]:\n",
        "        src = os.path.join(src_path, fname)\n",
        "        dst = os.path.join(dst_path, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "# Distributes images into training, validation, and test directories for each class\n",
        "for class_label in range(CLASS_COUNT):\n",
        "    class_name = CLASS_LIST[class_label]\n",
        "    class_files = os.listdir(os.path.join(IMAGE_PATH, class_name))\n",
        "    total_images = len(class_files)\n",
        "\n",
        "    # Uses 60% of images for training, 20% for validation, and 20% for testing\n",
        "    train_end = int(total_images * 0.6)\n",
        "    validation_end = train_end + int(total_images * 0.2)\n",
        "\n",
        "    create_dataset(IMAGE_PATH, train_dir, class_name, 0, train_end)\n",
        "    create_dataset(IMAGE_PATH, validation_dir, class_name, train_end, validation_end)\n",
        "    create_dataset(IMAGE_PATH, test_dir, class_name, validation_end, total_images)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints the number of cat and dog images in the training sample\n",
        "print('Number of cats %s, number of dogs %s in the training sample' \\\n",
        "      % (\n",
        "          len(os.listdir(os.path.join(train_dir, 'cats'))),\n",
        "          len(os.listdir(os.path.join(train_dir, 'dogs')))\n",
        "         )\n",
        "      )\n",
        "\n",
        "# Prints the number of cat and dog images in the validation sample\n",
        "print('Number of cats %s, number of dogs %s in the training sample' \\\n",
        "      % (\n",
        "          len(os.listdir(os.path.join(validation_dir, 'cats'))),\n",
        "          len(os.listdir(os.path.join(validation_dir, 'dogs')))\n",
        "         )\n",
        "      )\n",
        "\n",
        "# Prints the number of cat and dog images in the test sample\n",
        "print('Number of cats %s, number of dogs %s in the training sample' \\\n",
        "      % (\n",
        "          len(os.listdir(os.path.join(test_dir, 'cats'))),\n",
        "          len(os.listdir(os.path.join(test_dir, 'dogs')))\n",
        "         )\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfAbU4BMPfli",
        "outputId": "3fd56d33-c973-4dfa-e160-c3a68e87a389"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cats 2400, number of dogs 2403 in the training sample\n",
            "Number of cats 800, number of dogs 801 in the training sample\n",
            "Number of cats 800, number of dogs 801 in the training sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import MobileNet\n",
        "from keras import optimizers\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
        "from keras.models import Model\n",
        "\n",
        "# Function to create a сustom model\n",
        "def model_maker():\n",
        "    IMG_WIDTH = 150\n",
        "    IMG_HEIGHT = 150\n",
        "\n",
        "    # Initializes MobileNet\n",
        "    base_model = MobileNet(include_top=False, input_shape = (IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "\n",
        "    # Freezes all layers in the base model to prevent them from being updated during training\n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Defines the custom layers to be added on top of the base model\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(2, activation='softmax')(custom_model)\n",
        "\n",
        "    # Unfreezes the entire base model for fine-tuning\n",
        "    base_model.trainable = True #\n",
        "    set_trainable = False\n",
        "    for layer in base_model.layers:\n",
        "      if layer.name == 'conv_pw_10':\n",
        "          set_trainable = True #\n",
        "      if set_trainable:\n",
        "          layer.trainable = True\n",
        "      else:\n",
        "          layer.trainable = False\n",
        "\n",
        "    # Returns the complete model\n",
        "    return Model(inputs=input, outputs=predictions)"
      ],
      "metadata": {
        "id": "V6oE-kylRHg3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "\n",
        "# Initializes the model using the model_maker function\n",
        "model = model_maker()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am99CKkyQ3FX",
        "outputId": "6c6fce2c-bec9-4b31-c973-cd8f35988cc9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "# generator for training sample\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "# generator for verification sample\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# generation of pictures from the folder for training sample\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# generation of pictures from the folder for verification sample\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# model compilation\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# model training\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGYDskXkQgbV",
        "outputId": "53502d9a-f2dd-4432-bc83-ff9c7b715903"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4803 images belonging to 2 classes.\n",
            "Found 1601 images belonging to 2 classes.\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-2373297289d9>:41: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 78s 737ms/step - loss: 0.3657 - accuracy: 0.8535 - val_loss: 0.0988 - val_accuracy: 0.9630\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 73s 724ms/step - loss: 0.2856 - accuracy: 0.8830 - val_loss: 0.0986 - val_accuracy: 0.9580\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 70s 703ms/step - loss: 0.2293 - accuracy: 0.9025 - val_loss: 0.0923 - val_accuracy: 0.9630\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 77s 772ms/step - loss: 0.2149 - accuracy: 0.9055 - val_loss: 0.0893 - val_accuracy: 0.9630\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 70s 703ms/step - loss: 0.1816 - accuracy: 0.9319 - val_loss: 0.0845 - val_accuracy: 0.9670\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 70s 700ms/step - loss: 0.1722 - accuracy: 0.9300 - val_loss: 0.0877 - val_accuracy: 0.9680\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 77s 773ms/step - loss: 0.1724 - accuracy: 0.9330 - val_loss: 0.0702 - val_accuracy: 0.9740\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 77s 771ms/step - loss: 0.1660 - accuracy: 0.9314 - val_loss: 0.0783 - val_accuracy: 0.9730\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 79s 786ms/step - loss: 0.1306 - accuracy: 0.9515 - val_loss: 0.0596 - val_accuracy: 0.9760\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 70s 698ms/step - loss: 0.1324 - accuracy: 0.9481 - val_loss: 0.0790 - val_accuracy: 0.9700\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 77s 767ms/step - loss: 0.1551 - accuracy: 0.9390 - val_loss: 0.0842 - val_accuracy: 0.9700\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 77s 771ms/step - loss: 0.1210 - accuracy: 0.9516 - val_loss: 0.0627 - val_accuracy: 0.9760\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 71s 706ms/step - loss: 0.1171 - accuracy: 0.9576 - val_loss: 0.0739 - val_accuracy: 0.9690\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 78s 774ms/step - loss: 0.1364 - accuracy: 0.9445 - val_loss: 0.0685 - val_accuracy: 0.9730\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 70s 698ms/step - loss: 0.1312 - accuracy: 0.9486 - val_loss: 0.0601 - val_accuracy: 0.9740\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 77s 775ms/step - loss: 0.1153 - accuracy: 0.9520 - val_loss: 0.0701 - val_accuracy: 0.9720\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 71s 705ms/step - loss: 0.0930 - accuracy: 0.9635 - val_loss: 0.0874 - val_accuracy: 0.9680\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 79s 787ms/step - loss: 0.1076 - accuracy: 0.9605 - val_loss: 0.1005 - val_accuracy: 0.9590\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 77s 770ms/step - loss: 0.1018 - accuracy: 0.9597 - val_loss: 0.0675 - val_accuracy: 0.9730\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 77s 769ms/step - loss: 0.1060 - accuracy: 0.9622 - val_loss: 0.0561 - val_accuracy: 0.9790\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 77s 775ms/step - loss: 0.1148 - accuracy: 0.9556 - val_loss: 0.0699 - val_accuracy: 0.9680\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 77s 769ms/step - loss: 0.1026 - accuracy: 0.9592 - val_loss: 0.0452 - val_accuracy: 0.9800\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 77s 769ms/step - loss: 0.1035 - accuracy: 0.9592 - val_loss: 0.0598 - val_accuracy: 0.9770\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 70s 695ms/step - loss: 0.0945 - accuracy: 0.9662 - val_loss: 0.0576 - val_accuracy: 0.9770\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 78s 782ms/step - loss: 0.0947 - accuracy: 0.9630 - val_loss: 0.0609 - val_accuracy: 0.9740\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 78s 776ms/step - loss: 0.0851 - accuracy: 0.9695 - val_loss: 0.0547 - val_accuracy: 0.9800\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 78s 778ms/step - loss: 0.0694 - accuracy: 0.9705 - val_loss: 0.0517 - val_accuracy: 0.9760\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 77s 773ms/step - loss: 0.0819 - accuracy: 0.9687 - val_loss: 0.0439 - val_accuracy: 0.9800\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 71s 712ms/step - loss: 0.0777 - accuracy: 0.9720 - val_loss: 0.0492 - val_accuracy: 0.9800\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 78s 784ms/step - loss: 0.0813 - accuracy: 0.9685 - val_loss: 0.0550 - val_accuracy: 0.9780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model estimation based on test data\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=6)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huV85XhC5FKu",
        "outputId": "21ee7b98-8276-482d-d442-ddd268c8e56a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1601 images belonging to 2 classes.\n",
            "6/6 [==============================] - 2s 295ms/step - loss: 0.0251 - accuracy: 0.9917\n",
            "Test accuracy: 0.9916666746139526\n"
          ]
        }
      ]
    }
  ]
}